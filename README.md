# Apache spark
## A fast and general engine for large scale data prcessing.
Four most use cases of spark:

a. speed
b. Ease of use
c. Genarality
d. Platform agnostic

## How Apache Spark is related to Hadoop ?

Integration: Spark is designed to work with the Hadoop Distributed File System (HDFS) and other Hadoop components such as YARN (Yet Another Resource Negotiator), which provides resource management and scheduling capabilities.

Complementary technologies: Spark and Hadoop are complementary technologies that can work together to process large datasets. Hadoop provides distributed storage and batch processing capabilities, while Spark provides real-time processing, advanced analytics, and machine learning capabilities.

Shared ecosystem: Spark and Hadoop share a common ecosystem of tools and technologies such as Hive, Pig, and HBase, which can be used together to process and analyze large datasets.

Common deployment models: Spark can be deployed in the same cluster as Hadoop, using the same set of nodes and resources, making it easier to manage and scale both technologies together.

Overall, Spark and Hadoop are both part of the Big Data ecosystem and are often used together to process large datasets and perform advanced analytics.

## Core components of Spark
<img width="492" alt="image" src="https://user-images.githubusercontent.com/85188079/222244851-a219196b-3c55-4470-a016-55ff187ca268.png">



